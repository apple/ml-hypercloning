#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

import os

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from hypercloning import cloneModel

hf_token = os.environ["HF_TOKEN"]
example_models = [
    "meta-llama/Llama-3.2-1B",
    "google/gemma-2-2b-it",
    "meta-llama/Llama-2-7b-hf",
    "allenai/OLMO-1b",
    "google/gemma-2b",
    "EleutherAI/pythia-410m-deduped",
    "facebook/opt-350m",
]


def main():
    query = "The capital of France is"
    device = "cpu"
    for hf_path in example_models:
        print(f"\n\n\n##########################{hf_path}##########################")
        src_network = AutoModelForCausalLM.from_pretrained(
            hf_path, trust_remote_code=True, torch_dtype=torch.float32, token=hf_token
        )
        print(f"******** source network:\n{src_network}\n")
        dst_network = cloneModel(
            src_network, embedding_dim_multiplier=2, up_project_multiplier=1, snr_db=10
        )
        print(f"******** target network:\n{dst_network}\n")
        src_network.to(device)
        dst_network.to(device)
        tokenizer = AutoTokenizer.from_pretrained(hf_path, token=hf_token)
        inputs = tokenizer(query, return_tensors="pt")
        outputs = src_network.generate(
            inputs["input_ids"].to(device),
            max_length=50,
            num_return_sequences=1,
            do_sample=False,
        )
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        outputs = dst_network.generate(
            inputs["input_ids"].to(device),
            max_length=50,
            num_return_sequences=1,
            do_sample=False,
        )
        generated_text2 = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(
            f"******** generated by source network: \n {generated_text}\n ******** \
            generated by target network: \n {generated_text2}"
        )


if __name__ == "__main__":
    main()
